"""Fetch all fund histories via dffc and save each to PKL.

Usage:
    python fetch_all_funds.py \
            --output app/data/fund \
            --start 0 --end now \
            --start-row 1 --limit 0 \
            --max-workers 4 --min-interval 0.6 \
            --overwrite

Defaults:
    - output directory: app/data/fund
    - date range: until now
    - skip existing files unless --overwrite is set
    - 4 concurrent workers with a 0.6 s global interval between requests

Notes:
    - The script rate-limits all threads collectively to reduce the chance of
        triggering Eastmoney's anti-scraping protections.
    - Adjust --max-workers and --min-interval carefully; higher concurrency
        should always be paired with a longer interval.
    - All fund codes are read from fund_meta.csv (generated by code_crawl.py).
    - All failure messages are appended to output/error.log for later review.
    - Use --start-row / --limit to resume long downloads from any row slice.
"""

from __future__ import annotations

import argparse
import sys
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Optional

import pandas as pd
import dffc
from dffc.data_provider.eastmoney_provider import EastMoneyFundProvider
from vectorbt.utils.datetime_ import to_tzaware_datetime
from tqdm import tqdm

APP_DIR = Path(__file__).resolve().parents[1]
UTIL_DIR = APP_DIR / "_utils"
DEFAULT_OUTPUT = APP_DIR / "data" / "fund"


THREAD_LOCAL = threading.local()


def _get_provider() -> EastMoneyFundProvider:
    provider = getattr(THREAD_LOCAL, "provider", None)
    if provider is None:
        provider = EastMoneyFundProvider()
        THREAD_LOCAL.provider = provider
    return provider


def _rate_limit(context: dict) -> None:
    interval = context["min_interval"]
    if interval <= 0:
        return
    while True:
        with context["gate_lock"]:
            now = time.monotonic()
            wait = context["next_time"] - now
            if wait <= 0:
                context["next_time"] = now + interval
                return
        time.sleep(min(wait, interval))


def _log_failure(message: str, context: dict) -> None:
    print(message, file=sys.stderr)
    with context["log_lock"]:
        with context["error_log"].open("a", encoding="utf-8") as fp:
            fp.write(message + "\n")


def _fetch_one(code: str, plan: dict, context: dict) -> bool:
    out_path = plan["output"] / f"{code}.pkl"
    if out_path.exists() and not plan["overwrite"]:
        return False
    _rate_limit(context)
    try:
        fund_data = dffc.FundData.download(
            symbols=code,
            provider=_get_provider(),
            start=plan["start"],
            end=plan["end"],
        )
    except Exception as exc:  # pragma: no cover
        _log_failure(f"[FAIL] {code}: {exc}", context)
        return False
    fund_data.save(out_path)
    return True


def _run_fetch(plan: dict, codes: list[str]) -> int:
    context = {
        "min_interval": plan["min_interval"],
        "gate_lock": threading.Lock(),
        "next_time": 0.0,
        "log_lock": threading.Lock(),
        "error_log": plan["output"] / "error.log",
    }
    context["error_log"].touch(exist_ok=True)
    progress = tqdm(total=len(codes), desc="Fetching funds", unit="fund")
    saved = 0
    with ThreadPoolExecutor(max_workers=plan["max_workers"]) as pool:
        future_map = {pool.submit(_fetch_one, code, plan, context): code for code in codes}
        for future in as_completed(future_map):
            code = future_map[future]
            try:
                saved += bool(future.result())
            except Exception as exc:  # pragma: no cover
                _log_failure(f"[FAIL] {code}: {exc}", context)
            finally:
                progress.update(1)
    progress.close()
    return saved


def load_fund_codes(meta_path: Path) -> list[str]:
    if not meta_path.exists():
        raise FileNotFoundError(f"fund_meta.csv not found at {meta_path}")
    df = pd.read_csv(meta_path, dtype=str)
    if "code" not in df.columns:
        raise ValueError("fund_meta.csv missing 'code' column")
    return sorted(df["code"].dropna().astype(str).str.strip().unique())


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Fetch all funds to PKL")
    parser.add_argument("--output", type=Path, default=DEFAULT_OUTPUT, help="Output directory for PKL files")
    parser.add_argument("--start", type=str, default=0, help="Start date (YYYY-MM-DD) or 0 for earliest")
    parser.add_argument("--end", type=str, default="now", help="End date (YYYY-MM-DD or now)")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing PKL files")
    parser.add_argument("--limit", type=int, default=0, help="Optional limit on number of funds for testing")
    parser.add_argument("--start-row", type=int, default=1, help="1-based row index in fund_meta.csv to start from")
    parser.add_argument("--max-workers", type=int, default=4, help="Number of concurrent download workers")
    parser.add_argument(
        "--min-interval",
        type=float,
        default=0.6,
        help="Minimum seconds between outward requests across all threads",
    )
    return parser.parse_args()


def build_plan(args: argparse.Namespace) -> dict:
    return {
        "output": args.output,
        "start": to_tzaware_datetime(args.start),
        "end": to_tzaware_datetime(args.end),
        "overwrite": args.overwrite,
        "limit": max(0, args.limit),
        "start_row": max(1, args.start_row),
        "max_workers": max(1, args.max_workers),
        "min_interval": max(0.0, args.min_interval),
    }


def main() -> None:
    plan = build_plan(parse_args())
    plan["output"].mkdir(parents=True, exist_ok=True)

    codes = load_fund_codes(UTIL_DIR / "fund_meta.csv")
    
    # start_row is 1-based so convert to zero-based index before slicing
    start_idx = max(0, plan["start_row"] - 1)
    if start_idx >= len(codes):
        print("start_row exceeds available codes; nothing to fetch.")
        return
    codes = codes[start_idx:]
    if plan["limit"]:
        codes = codes[: plan["limit"]]

    saved = _run_fetch(plan, codes)
    print(f"Done. Total funds: {len(codes)}, saved: {saved}, output: {plan['output']}")


if __name__ == "__main__":
    main()
